{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Waypoint Navigation Using Multi-Goal-Conditioned Policy Search, And Learned Termination Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "Our aim is to learn a policy that maps an arbitrary number of waypoints to smooth navigation behaviour between those waypoints. Whereas previous work [] has made use of path-planning policies to learn complex maneuvers, we believe that this is less beneficial than using waypoints for several key reasons:\n",
    "\n",
    "1. Most mission-planning software still uses navigation between waypoints as the default paradigm. Motion planning is generally more costly, and limited to applications in which the full environment is known.\n",
    "2. \n",
    "\n",
    "## 2. Background\n",
    "\n",
    "### 2.1 Policy Search\n",
    "We focus on the class of methods known as Monte Carlo policy gradients. We assume a Markov Decision Process, in which we denote a trajectory $\\tau$ as an ordered set of events $\\{s_{0}, a_{0}, r_{0}, s_{1}, ..., s_{T} \\}$ for actions $a \\in \\mathcal{A}$, states $s \\in \\mathcal{S}$, and rewards $r \\in \\mathcal{R}$, where $t$ denotes an instantaneous point in time, and $T$ is the horizon. We assume a deterministic transition function given by the aircraft's flight dynamics.\n",
    "\n",
    "Policy search algorithms learn a mapping $\\pi_{\\theta}:\\mathcal{S} \\rightarrow \\mathcal{A}$, known as a policy, and adjust the policy's weights to maximize the expected return of a trajectory: \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:state_value_function}\n",
    "    V^{\\pi}(s_t) = \\mathbb{E}_{\\pi}\\left[\\sum_{t}^{T-1}\\gamma^{t}r_{t}|s_{t}=s\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:state_action_value}\n",
    "    Q^{\\pi}(s_t,a_t) = \\mathbb{E}_{\\pi}\\left[\\sum_{t}^{T-1}\\gamma^{t}r_{t}|s_{t}=s, a_{t}=a\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Where $V^{\\pi}(s)$ and $Q^{\\pi}(s,a)$ are the state and state-action value functions, respectively, that map $Q:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$, and $V:\\mathcal{S}\\rightarrow\\mathbb{R}$. $\\gamma \\in [0,1]$ is a discount factor that more heavily weights immediate rewards, and ensures convergence over an infinite horizon. The return is typically optimized using gradient ascent, in which the gradient is approximated using the score function estimator:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:policy_gradient}\n",
    "    \\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log{\\pi_{\\theta}(a_{t}|s_{t})}\\Phi_{t}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "In practice, Monte Carlo trajectory roll-outs are conducted under the current policy, and an empirical estimate of $Q^{\\pi}(s,a)$ (denoted $\\hat{Q}^{\\pi}(s,a)$) is calculated using Equation \\ref{eqn:state_action_value}. Equation \\ref{eqn:policy_gradient} is then used to calculate the gradient, where $\\Phi_{t}$ is typically the empirical advantage estimate $\\hat{A}^{\\pi}(s,a)=\\hat{Q}^{\\pi}(s,a)-V_{\\phi}^{\\pi}(s)$. $V_{\\phi}^{\\pi}(s)$ is a parameterized function that estimates Equation \\ref{eqn:state_value_function}, and is trained on the empirical returns from trajectory roll-outs.\n",
    "\n",
    "Since Equation \\ref{eqn:policy_gradient} is a high-variance estimator, modern policy gradient algorithms will often optimize the surrogate objective:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:modern_pg}\n",
    "\\begin{aligned}\n",
    "    & \\text{maximize:}\n",
    "    & & \\mathbb{E}_{s\\sim\\pi^{\\prime}, a\\sim\\pi} \\left[\\sum_{t=0}^{T-1} \\frac{\\pi(a|s)}{\\pi^{\\prime}(a|s)} A^{\\pi^{\\prime}}(s_{t},a_{t})  \\right] \\\\\n",
    "    \\\\\n",
    "    & \\text{such that:} \n",
    "    & & DKL(\\pi || \\pi^{\\prime}) \\leq \\delta\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Equation \\ref{eqn:modern_pg} has the advantage of bounding the policy update to a trust region within which monotonic improvement can be guaranteed, thus improving learning stability. \\ref{eqn:modern_pg} can be optimized using either constrained optimization, which results in Trust Region Policy Optimization (TRPO) or a first order approximation as is done in Proximal Policy Optimization (PPO).\n",
    "\n",
    "In certain cases, it makes sense to split a single difficult task into multiple sub-tasks that are individually easier to learn, and then find some way of combining them together. This idea is the central focus of hierarchical reinforcement learning, typified by methods such as options, MAX-Q, parameterized skills, and feudal networks. Hierarchical RL modifies the standard RL formulation to include multiple sub-policies for individual skills, an initiation set $\\mathcal{I}$ that determines when a policy can be activated, and a termination condition $\\beta$ that dictates when a policy should cease running. Hierarchical RL can be combined with Universal Value Function Approximation (UVFA) to learn more general policies, typically using value function methods such as Q-learning, or online methods such as DDPG. We combine UVFAs with Monte Carlo policy gradients to learn more general policies, which we discuss in further detail in Section.\n",
    "\n",
    "A closely related but distinct idea is that of auxiliary learning, in which an agent is trained to complete auxiliary tasks in addition to a given primary task.\n",
    "\n",
    "### 2.2 Quadrotor Flight Dynamics\n",
    "We briefly cover quadrotor flight dynamics here to cover some of the unique challenges presented. We model \n",
    "\n",
    "In general, quadrotors do not need to face \"fowards\" into the direction of flight, though this is a constraint we would like to impose. This breaks many of the symmetries inherent in quadrotor flight.\n",
    "\n",
    "\n",
    "## 3. Proposed Methodology\n",
    "We pose this problem as a skill learning task, in which the same skill is repeatedly applied. This is in contrast to other hierarchical methods such as options, in which a fixed number of policies are provided at the start of training, and a set of abstracted \"skills\" is learned. Looking at Fig. X, we want to fly some arbitrary trajectory between a set of waypoints, in which the shortest path from A to B is simply a straight line, but the optimal path through the points involves modifying the straight line path in some small way.\n",
    "\n",
    "In principle, if we know how to get from points A to B, then we should also be able to get from B to C, so long as the underlying dynamics of the problem are preserved. To make use of this insight, we propose a sliding-window type approach, in which our agent moves to a given waypoint, and we then update the origin of the agent's coordinate frame to the new position. In this way, the agent is effectively solving the same problem repeatedly. This representation is in stark contrast to standard feature representations on learning tasks, which often involve normalizing inputs in some way.\n",
    "\n",
    "The advantages of this representation are numerous:\n",
    "\n",
    "1. We don't need to keep track of statistics, because by definition, we can always choose a point within some local horizon, and reset the agent's position to be the origin. Blowup due to the observation space is effectively impossible, since all inputs are bounded.\n",
    "2. \n",
    "\n",
    "The downsides of this representation are that it induces a discrete shift in the agent's observation space, which has a number of consequences. Though we will show these explicitly further on, we will describe them here a:\n",
    "\n",
    "1. \"Creeping\", in which the agent creeps up to a goal, but never quite triggers it.\n",
    "2. \"Jumping\", in which the agent \"jumps\" from goal to goal in a sharp manner, and doesn't exhibit the smooth flight that we desire.\n",
    "\n",
    "Furthermore, our new state representation requires careful sequencing of the shift in order to ensure that our agent is able to learn.\n",
    "\n",
    "We hypothesise that these problems are caused by the discrete shift in the agent's origin, and propose two fixes:\n",
    "\n",
    "1. A soft boundary that has a probability to trigger based on distance from the goal; and,\n",
    "2. Treating this as an auxiliary learning task, and adding a termination policy that learns when a given goal has been reached such that the agent should move to the next one. This is an auxiliary learning task because the agent must balance trying to maximize the return from the trajectory, and the reward for switching to the next goal when close to the current one.\n",
    "\n",
    "Knowing when a goal has been reached is an important component of human intelligence, and is thus of interest here.\n",
    "\n",
    "## 4. Implementations\n",
    "In the following block of code, we implement our termination policy by inheriting from our own TRPO-PEB class, and including some additional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/seanmorrison/anaconda/lib/python36.zip', '/Users/seanmorrison/anaconda/lib/python3.6', '/Users/seanmorrison/anaconda/lib/python3.6/lib-dynload', '/Users/seanmorrison/.local/lib/python3.6/site-packages', '/Users/seanmorrison/anaconda/lib/python3.6/site-packages', '/Users/seanmorrison/anaconda/lib/python3.6/site-packages/aeosa', '/Users/seanmorrison/anaconda/lib/python3.6/site-packages/contextlib2-0.5.5-py3.6.egg', '/Users/seanmorrison/anaconda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg', '/Users/seanmorrison/Desktop/Projects/Collisions/gym', '/Users/seanmorrison/gym-aero', '/Users/seanmorrison/anaconda/lib/python3.6/site-packages/paramz-0.7.3-py3.6.egg', '/Users/seanmorrison/anaconda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg', '/Users/seanmorrison/anaconda/lib/python3.6/site-packages/IPython/extensions', '/Users/seanmorrison/.ipython', '../', '/Users/seanmorrison/quadrotor']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quadrotor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b471c777caae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mquadrotor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrpo_peb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mquadrotor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'quadrotor'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/seanmorrison/quadrotor\")\n",
    "sys.path\n",
    "\n",
    "from quadrotor.algs.ind import trpo_peb\n",
    "from quadrotor import cfg\n",
    "\n",
    "class Terminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Terminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, 1)\n",
    "        self.score = nn.Linear(hidden_dim, output_dim)\n",
    "        self.value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def step(self, x, hidden=None):\n",
    "        hx, cx = self.lstm(x.unsqueeze(1), hidden)\n",
    "        score = F.softmax(self.score(hx.squeeze(1)), dim=-1)\n",
    "        value = self.value(hx.squeeze(1))\n",
    "        return score, value, cx\n",
    "\n",
    "    def forward(self, x, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(x)\n",
    "        scores = torch.zeros(steps, 1, 1)\n",
    "        values = torch.zeros(steps, 1, 1)\n",
    "        for i in range(steps):\n",
    "            if force or i == 0:\n",
    "                input = x[i]\n",
    "            score, value, hidden = self.step(input, hidden)\n",
    "            scores[i] = score\n",
    "            values[i] = value\n",
    "        return scores, values, hidden\n",
    "    \n",
    "class Agent(trpo_peb.TRPO):\n",
    "    def __init__(self, pi, beta, critic, terminator, params, GPU=False):\n",
    "        super(Agent, self).__init__(pi, beta, critic, params, GPU)\n",
    "        self.terminator = terminator\n",
    "    \n",
    "    def terminate(self, x, hidden=None):\n",
    "        score, value, hidden = self.terminator.step(x.view(1,-1), hidden)\n",
    "        dist = Categorical(score)\n",
    "        term = dist.sample()\n",
    "        logprob = dist.log_prob(term)\n",
    "        return term, value, hidden, logprob\n",
    "    \n",
    "    def update(self, crit_opt, term_opt, trajectory):\n",
    "        rewards = torch.stack(trajectory[\"rewards\"])\n",
    "        masks = torch.stack(trajectory[\"masks\"])\n",
    "        states = torch.stack(trajectory[\"states\"])\n",
    "        next_states = torch.stack(trajectory[\"next_states\"])\n",
    "        term_log_probs = torch.stack(trajectory[\"term_log_probs\"])\n",
    "        term_rews = torch.stack(trajectory[\"term_rew\"])\n",
    "        term_vals = torch.stack(trajectory[\"term_val\"]).squeeze(1)\n",
    "        term_hiddens = trajectory[\"hiddens\"]\n",
    "        hxs = torch.stack([x[0] for x in term_hiddens])\n",
    "        cxs = torch.stack([x[1] for x in term_hiddens])\n",
    "        \n",
    "        term_returns = self.__Tensor(actions.size(0),1)\n",
    "        term_deltas = self.__Tensor(actions.size(0),1)\n",
    "        term_advantages = self.__Tensor(actions.size(0),1)\n",
    "        \n",
    "        term_prev_return = 0\n",
    "        term_prev_value = 0\n",
    "        term_prev_advantage = 0\n",
    "        \n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            if masks[i] == 0:\n",
    "                _, term_next_val, _, _ = self.terminate(next_states[i].unsqueeze(0), (hxs[i], cxs[i]))\n",
    "                term_prev_return = term_next_val.item()\n",
    "                term_prev_value = term_next_val.item()\n",
    "            \n",
    "            term_returns[i] = rewards[i]+term_rews[i]+self.__gamma*term_prev_return*masks[i]\n",
    "            term_deltas[i] = rewards[i]+term_rews[i]+self.__gamma*term_prev_value*masks[i]-term_vals.data[i]\n",
    "            term_advantages[i] = term_deltas[i]+self.__gamma*self.__tau*term_prev_advantage*masks[i]\n",
    "            \n",
    "            term_prev_return = term_returns[i, 0]\n",
    "            term_prev_value = term_vals.data[i, 0]\n",
    "            term_prev_advantage = term_advantages[i, 0]\n",
    "            \n",
    "        term_returns = (term_returns-term_returns.mean())/(term_returns.std()+1e-10)\n",
    "        term_advantages = (term_advantages-term_advantages.mean())/(term_advantages.std()+1e-10)\n",
    "        \n",
    "        # update terminator\n",
    "        term_opt.zero_grad()\n",
    "        term_crit_loss = F.smooth_l1_loss(term_vals, term_returns)\n",
    "        term_pol_loss = -term_log_probs*term_advantages \n",
    "        term_loss = term_pol_loss+term_crit_loss\n",
    "        term_loss = term_loss.mean()\n",
    "        term_loss.backward()\n",
    "        term_opt.step()\n",
    "        \n",
    "        # call the base update function\n",
    "        super(Agent, self).update(term_opt, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will inherit from the basic hierarchical task to include our termination policy step. For simplicity, we will make all of the changes we need in the step function, and return the termination policy reward in the info dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class TrajectoryEnvTerm(gym.envs.TrajectoryEnv):\n",
    "    def __init__(self):\n",
    "        super(TrajectoryEnvTerm, self).__init__()\n",
    "    \n",
    "    def next_goal(self, state):\n",
    "        xyz, zeta, uvw, pqr = state\n",
    "        if not self.goal >= len(self.goal_list)-1:\n",
    "            self.time_state = float(self.T)\n",
    "            self.t = 0\n",
    "            self.datum = xyz.copy()\n",
    "            self.goal += 1\n",
    "            self.goal_xyz = self.goal_list[self.goal]\n",
    "            self.goal_zeta_sin = self.zeta_list[self.goal][0]\n",
    "            self.goal_zeta_cos = self.zeta_list[self.goal][1]\n",
    "        if self.goal_next >= len(self.goal_list)-1:\n",
    "            self.goal_xyz_next = np.zeros((3,1))\n",
    "            self.goal_zeta_sin_next = np.zeros((3,1))\n",
    "            self.goal_zeta_cos_next = np.zeros((3,1))\n",
    "        else:\n",
    "            self.goal_next += 1\n",
    "            self.goal_xyz_next = self.goal_list[self.goal_next]\n",
    "            self.goal_zeta_sin_next = self.zeta_list[self.goal_next][0]\n",
    "            self.goal_zeta_cos_next = self.zeta_list[self.goal_next][1]\n",
    "        \n",
    "        s_zeta = np.sin(zeta)\n",
    "        c_zeta = np.cos(zeta)\n",
    "        curr_dist = xyz-self.goal_xyz+self.datum\n",
    "        curr_att_sin = s_zeta-self.goal_zeta_sin\n",
    "        curr_att_cos = c_zeta-self.goal_zeta_cos\n",
    "        curr_vel = uvw-self.goal_uvw\n",
    "        curr_ang = pqr-self.goal_pqr\n",
    "\n",
    "        # magnitude of the distance from the goal\n",
    "        dist_hat = np.linalg.norm(curr_dist)\n",
    "        att_hat_sin = np.linalg.norm(curr_att_sin)\n",
    "        att_hat_cos = np.linalg.norm(curr_att_cos)\n",
    "        vel_hat = np.linalg.norm(curr_vel)\n",
    "        ang_hat = np.linalg.norm(curr_ang)\n",
    "\n",
    "        # save new magnitudes\n",
    "        self.dist_norm = dist_hat\n",
    "        self.att_norm_sin = att_hat_sin\n",
    "        self.att_norm_cos = att_hat_cos\n",
    "        self.vel_norm = vel_hat\n",
    "        self.ang_norm = ang_hat\n",
    "\n",
    "        # save new vectors\n",
    "        self.vec_xyz = curr_dist\n",
    "        self.vec_zeta_sin = curr_att_sin\n",
    "        self.vec_zeta_cos = curr_att_cos\n",
    "        self.vec_uvw = curr_vel\n",
    "        self.vec_pqr = curr_ang\n",
    "        \n",
    "    def step(self, action, term):\n",
    "        rpm_command = self.trim_np+action*self.bandwidth\n",
    "        for _ in self.steps:\n",
    "            xs, zeta, uvw, pqr = self.iris.step(rpm_command)\n",
    "        self.time_state -= self.ctrl_dt\n",
    "        xyz = xs.copy()-self.datum\n",
    "        sin_zeta = np.sin(zeta)\n",
    "        cos_zeta = np.cos(zeta)\n",
    "        current_rpm = (self.iris.get_rpm()/self.action_bound[1]).tolist()\n",
    "        next_position = xyz.T.tolist()[0]\n",
    "        next_attitude = sin_zeta.T.tolist()[0]+cos_zeta.T.tolist()[0]\n",
    "        next_velocity = uvw.T.tolist()[0]+pqr.T.tolist()[0]\n",
    "        next_state = next_position+next_attitude+next_velocity\n",
    "        info = self.reward((xyz, zeta, uvw, pqr), self.trim_np+action*self.bandwidth)\n",
    "        term_rew = 0.\n",
    "        if term == 1:\n",
    "            d1 = self.dist_norm\n",
    "            term_rew = 15*exp(-15.*(d1**2))-7.5\n",
    "            self.next_goal((xyz, zeta, uvw, pqr))\n",
    "        done = self.terminal()\n",
    "        reward = sum(info)\n",
    "        position_goal = self.vec_xyz.T.tolist()[0]\n",
    "        attitude_goal = self.vec_zeta_sin.T.tolist()[0]+self.vec_zeta_cos.T.tolist()[0]\n",
    "        velocity_goal = self.vec_uvw.T.tolist()[0]+self.vec_pqr.T.tolist()[0]\n",
    "        goals = position_goal+attitude_goal+velocity_goal\n",
    "        next_position_goal = (xyz-self.goal_xyz_next).T.tolist()[0]\n",
    "        next_attitude_goal = (sin_zeta-self.goal_zeta_sin_next).T.tolist()[0]+(cos_zeta-self.goal_zeta_cos_next).T.tolist()[0]\n",
    "        next_velocity_goal = self.vec_uvw.T.tolist()[0]+self.vec_pqr.T.tolist()[0]\n",
    "        next_goals = next_position_goal+next_attitude_goal+next_velocity_goal\n",
    "        next_state = next_state+current_rpm+goals+next_goals+[self.time_state]\n",
    "        self.prev_action = rpm_command\n",
    "        self.prev_uvw = uvw\n",
    "        self.prev_pqr = pqr\n",
    "        self.t += 1\n",
    "        return next_state, reward, done, {\"dist_rew\": info[0], \n",
    "                                        \"dir_rew\": info[1], \n",
    "                                        \"vel_rew\": info[2], \n",
    "                                        \"ang_rew\": info[3],\n",
    "                                        \"ctrl_rew\": info[4],\n",
    "                                        \"time_rew\": info[5],\n",
    "                                        \"term_rew\": term_rew}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will write two basic training loops below. The first is for the standard agent, and the second is for the agent that has been modified with the termination policy. Aside from querying the termination policy and passing the termination reward to the update function, both algorithms are exactly the same. Similarly, we measure them using the same reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    interval_avg = []\n",
    "    avg = 0\n",
    "    for ep in range(1, self.__iterations+1):\n",
    "        s_, a_, ns_, r_, lp_, t_lp_, masks = [], [], [], [], [], [], []\n",
    "        num_steps = 1\n",
    "        reward_batch = 0\n",
    "        num_episodes = 0\n",
    "        while num_steps < batch_size+1:\n",
    "            state = env.reset()\n",
    "            state = Tensor(state)\n",
    "            reward_sum = 0\n",
    "            t = 0\n",
    "            hidden = None\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, log_prob = agent.select_action(state)\n",
    "                next_state, reward, done, info = env.step(action.cpu().data.numpy())\n",
    "                reward_sum += reward\n",
    "                next_state = Tensor(next_state)\n",
    "                \n",
    "                reward = Tensor([reward])\n",
    "                \n",
    "                s_.append(state)\n",
    "                ns_.append(next_state)\n",
    "                a_.append(action)\n",
    "                r_.append(reward)\n",
    "                lp_.append(log_prob)\n",
    "                masks.append(Tensor([not done]))\n",
    "                \n",
    "                state = next_state\n",
    "                t += 1\n",
    "            num_steps += t\n",
    "            num_episodes += 1\n",
    "            reward_batch += reward_sum\n",
    "            \n",
    "        reward_batch /= num_episodes\n",
    "        interval_avg.append(reward_batch)\n",
    "        avg = (avg*(ep-1)+reward_batch)/ep    \n",
    "        trajectory = {\n",
    "                    \"states\": s_,\n",
    "                    \"actions\": a_,\n",
    "                    \"rewards\": r_,\n",
    "                    \"next_states\": ns_,\n",
    "                    \"masks\": masks,\n",
    "                    \"log_probs\": lp_\n",
    "                    }\n",
    "\n",
    "        agent.train(crit_opt, trajectory)\n",
    "        if ep % log_interval == 0:\n",
    "            interval = float(sum(interval_avg))/float(len(interval_avg))\n",
    "            print('Episode {}\\t Interval average: {:.3f}\\t Average reward: {:.3f}'.format(ep, interval, avg))\n",
    "            interval_avg = []\n",
    "    torch.save(agent, env.name+\".pth.tar\")\n",
    "\n",
    "def train_term():\n",
    "    interval_avg = []\n",
    "    avg = 0\n",
    "    for ep in range(1, self.__iterations+1):\n",
    "        s_, a_, ns_, r_, lp_, t_lp_, masks = [], [], [], [], [], [], []\n",
    "        t_r_, t_v_, t_h_ = [], [], []\n",
    "        num_steps = 1\n",
    "        reward_batch = 0\n",
    "        num_episodes = 0\n",
    "        while num_steps < batch_size+1:\n",
    "            state = env.reset()\n",
    "            state = Tensor(state)\n",
    "            reward_sum = 0\n",
    "            t = 0\n",
    "            hidden = None\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, log_prob = agent.select_action(state)\n",
    "                term, term_val, hidden, term_log_prob = agent.terminate(state, hidden)\n",
    "                next_state, reward, done, info = env.step(action.cpu().data.numpy(), term.cpu().item())\n",
    "                term_rew = info[\"term_rew\"]\n",
    "                reward_sum += reward\n",
    "                next_state = Tensor(next_state)\n",
    "                \n",
    "                reward = Tensor([reward])\n",
    "                term_rew = Tensor([term_rew])\n",
    "                \n",
    "                s_.append(state)\n",
    "                ns_.append(next_state)\n",
    "                a_.append(action)\n",
    "                r_.append(reward)\n",
    "                lp_.append(log_prob)\n",
    "                masks.append(Tensor([not done]))\n",
    "                \n",
    "                t_lp_.append(term_log_prob)\n",
    "                t_r_.append(term_rew)\n",
    "                t_v_.append(term_val)\n",
    "                t_h_.append(hidden)\n",
    "                \n",
    "                state = next_state\n",
    "                t += 1\n",
    "            num_steps += t\n",
    "            num_episodes += 1\n",
    "            reward_batch += reward_sum\n",
    "            \n",
    "        reward_batch /= num_episodes\n",
    "        interval_avg.append(reward_batch)\n",
    "        avg = (avg*(ep-1)+reward_batch)/ep    \n",
    "        trajectory = {\n",
    "                    \"states\": s_,\n",
    "                    \"actions\": a_,\n",
    "                    \"rewards\": r_,\n",
    "                    \"next_states\": ns_,\n",
    "                    \"masks\": masks,\n",
    "                    \"log_probs\": lp_,\n",
    "                    \"term_log_probs\": t_lp_,\n",
    "                    \"term_rew\": t_r_,\n",
    "                    \"term_val\": t_v_,\n",
    "                    \"hiddens\": t_h_\n",
    "                    }\n",
    "\n",
    "        agent.train(crit_opt, term_opt, trajectory)\n",
    "        if ep % log_interval == 0:\n",
    "            interval = float(sum(interval_avg))/float(len(interval_avg))\n",
    "            print('Episode {}\\t Interval average: {:.3f}\\t Average reward: {:.3f}'.format(ep, interval, avg))\n",
    "            interval_avg = []\n",
    "    torch.save(agent, env.name+\"-term.pth.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiments\n",
    "We conduct the following experiments:\n",
    "\n",
    "1. We test a standard agent on the task of flying between sequential waypoints. We vary the goal threshold to show that the performance of the agent is highly dependent on this single parameter, and that we can essentially \"hack\" the performance of the agent;\n",
    "2. We test our own architecture using learned termination conditions and show that it doesn't suffer from this problem;\n",
    "3. We compare the agents on the basis of acceleration to show that ours exhibits smoother flight;\n",
    "4. We test the termination gradient theorem, and show that it doesn't apply to our particular architecture;\n",
    "5. We test an option-critic agent, and show that it performs no better than the standard agent;\n",
    "6. We compare our waypoint-based agents with an agent that is trained to follow a path along a spline, and show that our method is more general by altering the trajectory length; and,\n",
    "7. We show that our method does not make effective use of future waypoints, but that -- given the right reward function -- the standard agent *can*.\n",
    "\n",
    "All agents are trained using the *exact same* rewards. To make this explicit, we include the reward function below, and will pass it to the environment as an argument:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(self, state, action):\n",
    "        \n",
    "        xyz, zeta, uvw, pqr = state\n",
    "        s_zeta = np.sin(zeta)\n",
    "        c_zeta = np.cos(zeta)\n",
    "        curr_dist = xyz-self.goal_xyz+self.datum\n",
    "        curr_att_sin = s_zeta-self.goal_zeta_sin\n",
    "        curr_att_cos = c_zeta-self.goal_zeta_cos\n",
    "        curr_vel = uvw-self.goal_uvw\n",
    "        curr_ang = pqr-self.goal_pqr\n",
    "\n",
    "        # magnitude of the distance from the goal\n",
    "        dist_hat = np.linalg.norm(curr_dist)\n",
    "        att_hat_sin = np.linalg.norm(curr_att_sin)\n",
    "        att_hat_cos = np.linalg.norm(curr_att_cos)\n",
    "        vel_hat = np.linalg.norm(curr_vel)\n",
    "        ang_hat = np.linalg.norm(curr_ang)\n",
    "        \n",
    "        guide_rew = 0.\n",
    "        \n",
    "        dist_rew = 100.*(self.dist_norm-dist_hat)\n",
    "        att_rew = 100*((self.att_norm_sin-att_hat_sin)+(self.att_norm_cos-att_hat_cos))\n",
    "        vel_rew = 50.*(self.vel_norm-vel_hat)\n",
    "        ang_rew = 50.*(self.ang_norm-ang_hat)\n",
    "\n",
    "        self.dist_norm = dist_hat\n",
    "        self.att_norm_sin = att_hat_sin\n",
    "        self.att_norm_cos = att_hat_cos\n",
    "        self.vel_norm = vel_hat\n",
    "        self.ang_norm = ang_hat\n",
    "\n",
    "        self.vec_xyz = curr_dist\n",
    "        self.vec_zeta_sin = curr_att_sin\n",
    "        self.vec_zeta_cos = curr_att_cos\n",
    "        self.vec_uvw = curr_vel\n",
    "        self.vec_pqr = curr_ang\n",
    "\n",
    "        # agent gets a negative reward for excessive action inputs\n",
    "        ctrl_rew = 0.\n",
    "        ctrl_rew -= np.sum(((action-self.trim_np)/self.action_bound[1])**2)\n",
    "        ctrl_rew -= np.sum((((action-self.prev_action)/self.action_bound[1])**2))\n",
    "        \n",
    "        # try to minimize acceleration to prioritize smooth flight\n",
    "        ctrl_rew -= 50.*np.sum((uvw-self.prev_uvw)**2)\n",
    "        ctrl_rew -= 50.*np.sum((pqr-self.prev_pqr)**2)\n",
    "\n",
    "        time_rew = 0.\n",
    "        return dist_rew, att_rew, vel_rew, ang_rew, ctrl_rew, time_rew, guide_rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our agents make use of the same state representation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "### 6.1 Experiment 1 -- Validating Standard Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_one = gym.make(\"TrajectoryEnv\")\n",
    "env_one.reward = reward_func\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = params[\"hidden_dim\"]\n",
    "\n",
    "def train_standard_agent():\n",
    "    pi = Actor(state_dim, hidden_dim, action_dim)\n",
    "    beta = Actor(state_dim, hidden_dim, action_dim)\n",
    "    critic = Critic(state_dim, hidden_dim, 1)\n",
    "    agent = trpo_peb.TRPO(pi, beta, critic, params[\"network_settings\"], GPU=cuda)\n",
    "    crit_opt = torch.optim.Adam(critic.parameters())\n",
    "    ep, rew = train()\n",
    "    return ep, rew\n",
    "\n",
    "ep, rew = train_standard_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Experiment 2 -- Learning Termination Conditions Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_two = TrajectoryEnvTerm()\n",
    "env_two.reward = reward_func\n",
    "\n",
    "def train_term_agent():\n",
    "    pi = Actor(state_dim, hidden_dim, action_dim)\n",
    "    beta = Actor(state_dim, hidden_dim, action_dim)\n",
    "    critic = Critic(state_dim, hidden_dim, 1)\n",
    "    terminator = Terminator(self.state_dim, self.hidden_dim, 2)\n",
    "    agent = Agent(pi, beta, critic, terminator, params[\"network_settings\"], GPU=cuda)\n",
    "    crit_opt = torch.optim.Adam(critic.parameters())\n",
    "    term_opt = torch.optim.Adam()\n",
    "    ep, rew = train_term()\n",
    "    return ep, rew\n",
    "\n",
    "ep, rew = train_term_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Experiment 3 -- Varying Goal Threshold Radius\n",
    "One issue with goal-based policies is the problem of knowing when a goal has been reached. Below, we show that we can essentially \"hack\" the return of the policy through careful selection of the goal threshold radius. This diminishes the usefulness of the reward as a measure of the agent's effectiveness. For this reason, we opt to use a measure of smoothness of the flight trajectory instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "episodes, rewards = [], []\n",
    "for r in radii:\n",
    "    env_one.goal_thresh = r\n",
    "    ep, rew = train_standard_agent\n",
    "    episodes.append(ep)\n",
    "    rewards.append(rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Analysis -- Visualizing Trajectory Derivatives and the Closest Distance to the Current Waypoint\n",
    "As shown above, measuring the agent's ability to reach each sequential goal is not the best measure of performance on this task, since we can arbitrarily tune the goal threshold to improve the return. In this section, we visualize both the trajectory derivatives -- to determine the smoothness of the path -- and the distance at which the termination policy switches to the next goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Experiment 4 -- Comparison With Option-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 -- Incorporating the Projected Gradient\n",
    "Since we are dealing with an auxiliary learning task, there are instances where the gradient of one task (such as terminating the current waypoint) may impact on the gradient of another (such as maximizing the overall reward of the flight policy). This is known as negative transfer, and is a central challenge in auxiliary learning tasks. One technique that has been proposed for solving this issue is known as the projected gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentProjected(trpo_peb.TRPO):\n",
    "    def __init__(self, pi, beta, critic, terminator, params, GPU=False):\n",
    "        super(AgentProjected, self).__init__(pi, beta, critic, params, GPU)\n",
    "        self.terminator = terminator\n",
    "    \n",
    "    def terminate(self, x, hidden=None):\n",
    "        score, value, hidden = self.terminator.step(x.view(1,-1), hidden)\n",
    "        dist = Categorical(score)\n",
    "        term = dist.sample()\n",
    "        logprob = dist.log_prob(term)\n",
    "        return term, value, hidden, logprob\n",
    "    \n",
    "    def update(self, crit_opt, term_opt, trajectory):\n",
    "        rewards = torch.stack(trajectory[\"rewards\"])\n",
    "        masks = torch.stack(trajectory[\"masks\"])\n",
    "        states = torch.stack(trajectory[\"states\"])\n",
    "        next_states = torch.stack(trajectory[\"next_states\"])\n",
    "        term_log_probs = torch.stack(trajectory[\"term_log_probs\"])\n",
    "        term_rews = torch.stack(trajectory[\"term_rew\"])\n",
    "        term_vals = torch.stack(trajectory[\"term_val\"]).squeeze(1)\n",
    "        term_hiddens = trajectory[\"hiddens\"]\n",
    "        hxs = torch.stack([x[0] for x in term_hiddens])\n",
    "        cxs = torch.stack([x[1] for x in term_hiddens])\n",
    "        \n",
    "        term_returns = self.__Tensor(actions.size(0),1)\n",
    "        term_deltas = self.__Tensor(actions.size(0),1)\n",
    "        term_advantages = self.__Tensor(actions.size(0),1)\n",
    "        \n",
    "        term_prev_return = 0\n",
    "        term_prev_value = 0\n",
    "        term_prev_advantage = 0\n",
    "        \n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            if masks[i] == 0:\n",
    "                _, term_next_val, _, _ = self.terminate(next_states[i].unsqueeze(0), (hxs[i], cxs[i]))\n",
    "                term_prev_return = term_next_val.item()\n",
    "                term_prev_value = term_next_val.item()\n",
    "            \n",
    "            term_returns[i] = rewards[i]+term_rews[i]+self.__gamma*term_prev_return*masks[i]\n",
    "            term_deltas[i] = rewards[i]+term_rews[i]+self.__gamma*term_prev_value*masks[i]-term_vals.data[i]\n",
    "            term_advantages[i] = term_deltas[i]+self.__gamma*self.__tau*term_prev_advantage*masks[i]\n",
    "            \n",
    "            term_prev_return = term_returns[i, 0]\n",
    "            term_prev_value = term_vals.data[i, 0]\n",
    "            term_prev_advantage = term_advantages[i, 0]\n",
    "            \n",
    "        term_returns = (term_returns-term_returns.mean())/(term_returns.std()+1e-10)\n",
    "        term_advantages = (term_advantages-term_advantages.mean())/(term_advantages.std()+1e-10)\n",
    "        \n",
    "        # update terminator\n",
    "        term_opt.zero_grad()\n",
    "        term_crit_loss = F.smooth_l1_loss(term_vals, term_returns)\n",
    "        term_pol_loss = -term_log_probs*term_advantages \n",
    "        term_loss = term_pol_loss+term_crit_loss\n",
    "        term_loss = term_loss.mean()\n",
    "        term_loss.backward()\n",
    "        term_opt.step()\n",
    "        \n",
    "        super(Agent, self).update(term_opt, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Termination Gradient Theorem\n",
    "We implement the termination gradient theorem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(trpo_peb.TRPO):\n",
    "    def __init__(self, pi, beta, critic, terminator, params, GPU=False):\n",
    "        super(Agent, self).__init__(pi, beta, critic, params, GPU)\n",
    "        self.terminator = terminator\n",
    "    \n",
    "    def terminate(self, x, hidden=None):\n",
    "        score, value, hidden = self.terminator.step(x.view(1,-1), hidden)\n",
    "        dist = Categorical(score)\n",
    "        term = dist.sample()\n",
    "        logprob = dist.log_prob(term)\n",
    "        return term, value, hidden, logprob\n",
    "    \n",
    "    def train(self, crit_opt, term_opt, trajectory):\n",
    "        rewards = torch.stack(trajectory[\"rewards\"])\n",
    "        masks = torch.stack(trajectory[\"masks\"])\n",
    "        states = torch.stack(trajectory[\"states\"])\n",
    "        next_states = torch.stack(trajectory[\"next_states\"])\n",
    "        term_log_probs = torch.stack(trajectory[\"term_log_probs\"])\n",
    "        term_rews = torch.stack(trajectory[\"term_rew\"])\n",
    "        term_vals = torch.stack(trajectory[\"term_val\"]).squeeze(1)\n",
    "        term_hiddens = trajectory[\"hiddens\"]\n",
    "        hxs = torch.stack([x[0] for x in term_hiddens])\n",
    "        cxs = torch.stack([x[1] for x in term_hiddens])\n",
    "        \n",
    "        term_returns = self.__Tensor(actions.size(0),1)\n",
    "        term_deltas = self.__Tensor(actions.size(0),1)\n",
    "        term_advantages = self.__Tensor(actions.size(0),1)\n",
    "        \n",
    "        term_prev_return = 0\n",
    "        term_prev_value = 0\n",
    "        term_prev_advantage = 0\n",
    "        \n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            if masks[i] == 0:\n",
    "                _, term_next_val, _, _ = self.terminate(next_states[i].unsqueeze(0), (hxs[i], cxs[i]))\n",
    "                term_prev_return = term_next_val.item()\n",
    "                term_prev_value = term_next_val.item()\n",
    "            \n",
    "            term_returns[i] = rewards[i]+term_rews[i]+self.__gamma*term_prev_return*masks[i]\n",
    "            term_deltas[i] = rewards[i]+term_rews[i]+self.__gamma*term_prev_value*masks[i]-term_vals.data[i]\n",
    "            term_advantages[i] = term_deltas[i]+self.__gamma*self.__tau*term_prev_advantage*masks[i]\n",
    "            \n",
    "            term_prev_return = term_returns[i, 0]\n",
    "            term_prev_value = term_vals.data[i, 0]\n",
    "            term_prev_advantage = term_advantages[i, 0]\n",
    "            \n",
    "        term_returns = (term_returns-term_returns.mean())/(term_returns.std()+1e-10)\n",
    "        term_advantages = (term_advantages-term_advantages.mean())/(term_advantages.std()+1e-10)\n",
    "        \n",
    "        # update terminator\n",
    "        term_opt.zero_grad()\n",
    "        term_crit_loss = F.smooth_l1_loss(term_vals, term_returns)\n",
    "        term_pol_loss = -term_log_probs*term_advantages \n",
    "        term_loss = term_pol_loss+term_crit_loss\n",
    "        term_loss = term_loss.mean()\n",
    "        term_loss.backward()\n",
    "        term_opt.step()\n",
    "        \n",
    "        super(Agent, self).update(term_opt, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Comparing with Motion-Planning-Based Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graphs that the motion-planning-based policy can indeed learn the task of flying between the waypoints. What is less obvious, however, is the difficulty the agent has in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion\n",
    "\n",
    "### 7.1 Limitations\n",
    "Presently, the agent is effective in learning smooth flight through sequences of obtuse angles, but switches prematurely when faced with acute angles. We believe this is a problem of the agent getting stuck in local minima, and that solving this problem will also help with utilizing future waypoints more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Related Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
